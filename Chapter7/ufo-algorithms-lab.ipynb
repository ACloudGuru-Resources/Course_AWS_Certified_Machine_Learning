{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFO Sightings Algorithms Lab\n",
    "\n",
    "The goal of this notebook is to build out models to use for predicting the legitimacy of a UFO sighting using the XGBoost and Linear Learner algorithm.\n",
    "\n",
    "What we plan on accompishling is the following:\n",
    "1. [Load dataset onto Notebook instance memory from S3](#Step-1:-Load-the-data-from-Amazon-S3)\n",
    "1. [Cleaning, transforming, analyize, and preparing the dataset](#Step-2:-Cleaning,-transforming,-analyize,-and-preparing-the-dataset)\n",
    "1. [Create and train our model (XGBoost)](#Step-3:-Creating-and-training-our-model-(XGBoost))\n",
    "1. [Create and train our model (Linear Learner)](#Step-4:-Creating-and-training-our-model-(Linear-Learner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's go ahead and import all the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from Amazon S3\n",
    "Let's get the UFO sightings data that is stored in S3 and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket='<INSERT_YOUR_BUCKET_NAME_HERE>'\n",
    "sub_folder = 'ufo_dataset'\n",
    "data_key = 'ufo_fullset.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning, transforming, analyize, and preparing the dataset\n",
    "This step is so important. It's crucial that we clean and prepare our data before we do anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check to see if there are any missing values\n",
    "missing_values = df.isnull().values.any()\n",
    "if(missing_values):\n",
    "    display(df[df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shape'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing values with the most common shape\n",
    "df['shape'] = df['shape'].fillna(df['shape'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's go ahead and start preparing our dataset by transforming some of the values into the correct data types. Here is what we are going to take care of.\n",
    "1. Convert the `reportedTimestamp` and `eventDate` to a datetime data types.\n",
    "1. Convert the `shape` and `weather` to a category data type.\n",
    "1. Map the `physicalEvidence` and `contact` from 'Y', 'N' to `0`, `1`.\n",
    "1. Convert the `researchOutcome` to a category data type (target attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reportedTimestamp'] = pd.to_datetime(df['reportedTimestamp'])\n",
    "df['eventDate'] = pd.to_datetime(df['eventDate'])\n",
    "\n",
    "df['shape'] = df['shape'].astype('category')\n",
    "df['weather'] = df['weather'].astype('category')\n",
    "\n",
    "df['physicalEvidence'] = df['physicalEvidence'].replace({'Y': 1, 'N': 0})\n",
    "df['contact'] = df['contact'].replace({'Y': 1, 'N': 0})\n",
    "\n",
    "df['researchOutcome'] = df['researchOutcome'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the data to see if we can find out any important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_cts = (df['contact'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings and Contact')\n",
    "ax.set_xlabel('Was contact made?')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "ax.set_xticklabels(['No', 'Yes'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_cts = (df['physicalEvidence'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings and Physical Evidence')\n",
    "ax.set_xlabel('Was there physical evidence?')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "ax.set_xticklabels(['No', 'Yes'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cts = (df['shape'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings by Shape')\n",
    "ax.set_xlabel('UFO Shape')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_cts = (df['weather'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings by Weather')\n",
    "ax.set_xlabel('Weather')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cts = (df['researchOutcome'].value_counts())\n",
    "m_ctsx = m_cts.index\n",
    "m_ctsy = m_cts.get_values()\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.barplot(x=m_ctsx, y=m_ctsy)\n",
    "ax.set_title('UFO Sightings and Research Outcome')\n",
    "ax.set_xlabel('Research Outcome')\n",
    "ax.set_ylabel('Number of Sightings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ufo_yr = df['eventDate'].dt.year  # series with the year exclusively\n",
    "\n",
    "## Set axes ##\n",
    "years_data = ufo_yr.value_counts()\n",
    "years_index = years_data.index  # x ticks\n",
    "years_values = years_data.get_values()\n",
    "\n",
    "## Create Bar Plot ##\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.xticks(rotation = 60)\n",
    "plt.title('UFO Sightings by Year')\n",
    "plt.ylabel('Number of Sightings')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "years_plot = sns.barplot(x=years_index[:60],y=years_values[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the columns that are not important. \n",
    "1. We can drop `sighting` becuase it is always 'Y' or Yes. \n",
    "1. Let's drop the `firstName` and `lastName` becuase they are not important in determining the `researchOutcome`.\n",
    "1. Let's drop the `reportedTimestamp` becuase when the sighting was reporting isn't going to help us determine the legitimacy of the sighting.\n",
    "1. We would need to create some sort of buckets for the `eventDate` and `eventTime`, like seasons for example, but since the distribution of dates is pretty even, let's go ahead and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['firstName', 'lastName', 'sighting', 'reportedTimestamp', 'eventDate', 'eventTime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply one-hot encoding\n",
    "1. We need to one-hot both the `weather` attribute and the `shape` attribute. \n",
    "1. We also need to transform or map the researchOutcome (target) attribute into numeric values. This is what the alogrithm is expecting. We can do this by mapping unexplained, explained, and probable to 0, 1, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's one-hot the weather and shape attribute\n",
    "df = pd.get_dummies(df, columns=['weather', 'shape'])\n",
    "\n",
    "# Let's replace the researchOutcome values with 0, 1, 2 for Unexplained, Explained, and Probable\n",
    "df['researchOutcome'] = df['researchOutcome'].replace({'unexplained': 0, 'explained': 1, 'probable': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomize and split the data into training, validation, and testing.\n",
    "1. First we need to randomize the data.\n",
    "1. Next Let's use 80% of the dataset for our training set.\n",
    "1. Then use 10% for validation during training.\n",
    "1. Finally we will use 10% for testing our model after it is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and randomize our data.\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Next, Let's split the data into a training, validation, and testing.\n",
    "rand_split = np.random.rand(len(df))\n",
    "train_list = rand_split < 0.8                       # 80% for training\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9) # 10% for validation\n",
    "test_list = rand_split >= 0.9                       # 10% for testing\n",
    "\n",
    " # This dataset will be used to train the model.\n",
    "data_train = df[train_list]\n",
    "\n",
    "# This dataset will be used to validate the model.\n",
    "data_val = df[val_list]\n",
    "\n",
    "# This dataset will be used to test the model.\n",
    "data_test = df[test_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's go ahead and rearrange our attributes so the first attribute is our target attribute `researchOutcome`. This is what AWS requires and the XGBoost algorithms expects. You can read all about it here in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost).\n",
    "\n",
    "After that we will go ahead and create those files on our Notebook instance (stored as CSV) and then upload them to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply moves the researchOutcome attribute to the first position before creating CSV files\n",
    "pd.concat([data_train['researchOutcome'], data_train.drop(['researchOutcome'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([data_val['researchOutcome'], data_val.drop(['researchOutcome'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# Next we can take the files we just stored onto our Notebook instance and upload them to S3.\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/xgboost_train/train.csv').upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/xgboost_validation/validation.csv').upload_file('validation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating and training our model (XGBoost)\n",
    "\n",
    "This is where the magic happens. We will get the ECR container hosted in ECR for the XGBoost algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, because we're training with the CSV file format, we'll create inputs that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/algorithms_lab/xgboost_train'.format(bucket), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/algorithms_lab/xgboost_validation'.format(bucket), content_type='csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start building out our model by using the SageMaker Python SDK and passing in everything that is required to create a XGBoost model.\n",
    "\n",
    "First I like to always create a specific job name.\n",
    "\n",
    "Next, we'll need to specify training parameters.\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data/model artifact\n",
    "1. [XGBoost Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)\n",
    "\n",
    "\n",
    "Finally, after everything is included and ready, then we can call the `.fit()` function which specifies the S3 location for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training job name\n",
    "job_name = 'ufo-xgboost-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))\n",
    "\n",
    "# Here is where the model artifact will be stored\n",
    "output_location = 's3://{}/algorithms_lab/xgboost_output'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(objective='multi:softmax',\n",
    "                        num_class=3,\n",
    "                        num_round=100)\n",
    "\n",
    "data_channels = {\n",
    "    'train': s3_input_train,\n",
    "    'validation': s3_input_validation\n",
    "}\n",
    "xgb.fit(data_channels, job_name=job_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Here is the location of the trained XGBoost model: {}/{}/output/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train our model we can see the default evaluation metric in the logs. The `merror` is used in multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). We want this to be minimized (so we want this to be super small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Creating and training our model (Linear Learner)\n",
    "\n",
    "Let's evaluate the Linear Learner algorithm as well. Let's go ahead and randomize the data again and get it ready for the Linear Leaner algorithm. We will also rearrange the columns so it is ready for the algorithm (it expects the first column to be the target attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rand_split = np.random.rand(len(df))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = rand_split >= 0.9\n",
    "\n",
    " # This dataset will be used to train the model.\n",
    "data_train = df[train_list]\n",
    "\n",
    "# This dataset will be used to validate the model.\n",
    "data_val = df[val_list]\n",
    "\n",
    "# This dataset will be used to test the model.\n",
    "data_test = df[test_list]\n",
    "\n",
    "# Breaks the datasets into attribute numpy.ndarray and the same for target attribute.  \n",
    "train_X = data_train.drop(columns='researchOutcome').as_matrix()\n",
    "train_y = data_train['researchOutcome'].as_matrix()\n",
    "\n",
    "val_X = data_val.drop(columns='researchOutcome').as_matrix()\n",
    "val_y = data_val['researchOutcome'].as_matrix()\n",
    "\n",
    "test_X = data_test.drop(columns='researchOutcome').as_matrix()\n",
    "test_y = data_test['researchOutcome'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's create recordIO file for the training data and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'ufo_sightings_train_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/linearlearner_train/{}'.format(train_file)).upload_fileobj(f)\n",
    "training_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_train/{}'.format(bucket, train_file)\n",
    "print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create recordIO file for the validation data and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/linearlearner_validation/{}'.format(validation_file)).upload_fileobj(f)\n",
    "validate_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_validation/{}'.format(bucket, validation_file)\n",
    "print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Alright we are good to go for the Linear Learner algorithm. Let's get everything we need from the ECR repository to call the Linear Learner algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner', \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training job name\n",
    "job_name = 'ufo-linear-learner-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))\n",
    "\n",
    "# Here is where the model-artifact will be stored\n",
    "output_location = 's3://{}/algorithms_lab/linearlearner_output'.format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start building out our model by using the SageMaker Python SDK and passing in everything that is required to create a Linear Learner model.\n",
    "\n",
    "First I like to always create a specific job name.\n",
    "\n",
    "Next, we'll need to specify training parameters.\n",
    "1. The `linear-learner` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data/model artifact\n",
    "1. [The input type (Pipe)](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html)\n",
    "1. [Linear Learner Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html)\n",
    "\n",
    "\n",
    "Finally, after everything is included and ready, then we can call the `.fit()` function which specifies the S3 location for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "# Setup the LinearLeaner algorithm from the ECR container\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess,\n",
    "                                       input_mode='Pipe')\n",
    "# Setup the hyperparameters\n",
    "linear.set_hyperparameters(feature_dim=22, # number of attributes (minus the researchOutcome attribute)\n",
    "                           predictor_type='multiclass_classifier', # type of classification problem\n",
    "                           num_classes=3)  # number of classes in out researchOutcome (explained, unexplained, probable)\n",
    "\n",
    "\n",
    "# Launch a training job. This method calls the CreateTrainingJob API call\n",
    "data_channels = {\n",
    "    'train': training_recordIO_protobuf_location,\n",
    "    'validation': validate_recordIO_protobuf_location\n",
    "}\n",
    "linear.fit(data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here is the location of the trained Linear Learner model: {}/{}/output/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we have two trained models to present to Mr. K. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
