{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFO Sightings Implementation and Operations Lab\n",
    "\n",
    "The goal of this notebook is to train and deploy our model into SageMaker online hosting with 1 variant. \n",
    "\n",
    "What we plan on accompishling is the following:\n",
    "1. [Load dataset onto Notebook instance memory from S3](#Step-1:-Load-the-data-from-Amazon-S3)\n",
    "1. [Cleaning, transforming and preparing the dataset](#Step-2:-Cleaning,-transforming-and-preparing-the-dataset)\n",
    "1. [Create and train our model (Linear Learner)](#Step-4:-Creating-and-training-our-model-(Linear-Learner))\n",
    "1. [Deploying the model into SageMaker hosting](#Step-4:-Deploying-the-model-into-SageMaker-hosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's go ahead and import all the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from Amazon S3\n",
    "Let's get the UFO sightings data that is stored in S3 and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket='<INSERT_BUCKET_NAME_HERE>'\n",
    "sub_folder = 'ufo_dataset'\n",
    "data_key = 'ufo_fullset.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning, transforming and preparing the dataset\n",
    "This step is so important. It's crucial that we clean and prepare our data before we do anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's go ahead and start preparing our dataset by transforming some of the values into the correct data types. Here is what we are going to take care of.\n",
    "1. Convert the `reportedTimestamp` and `eventDate` to a datetime data types.\n",
    "1. Convert the `shape` and `weather` to a category data type.\n",
    "1. Map the `physicalEvidence` and `contact` from 'Y', 'N' to `0`, `1`.\n",
    "1. Convert the `researchOutcome` to a category data type (target attribute).\n",
    "\n",
    "Let's also drop the columns that are not important. \n",
    "1. We can drop `sighting` becuase it is always 'Y' or Yes. \n",
    "1. Let's drop the `firstName` and `lastName` becuase they are not important in determining the `researchOutcome`.\n",
    "1. Let's drop the `reportedTimestamp` becuase when the sighting was reporting isn't going to help us determine the legitimacy of the sighting.\n",
    "1. We would need to create some sort of buckets for the `eventDate` and `eventTime`, like seasons for example, but since the distribution of dates is pretty even, let's go ahead and drop them.\n",
    "\n",
    "Finally, let's apply one-hot encoding\n",
    "1. We need to one-hot both the `weather` attribute and the `shape` attribute. \n",
    "1. We also need to transform or map the researchOutcome (target) attribute into numeric values. This is what the alogrithm is expecting. We can do this by mapping unexplained, explained, and probable to 0, 1, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing values with the most common shape\n",
    "df['shape'] = df['shape'].fillna(df['shape'].value_counts().index[0])\n",
    "\n",
    "df['reportedTimestamp'] = pd.to_datetime(df['reportedTimestamp'])\n",
    "df['eventDate'] = pd.to_datetime(df['eventDate'])\n",
    "\n",
    "df['shape'] = df['shape'].astype('category')\n",
    "df['weather'] = df['weather'].astype('category')\n",
    "\n",
    "df['physicalEvidence'] = df['physicalEvidence'].replace({'Y': 1, 'N': 0})\n",
    "df['contact'] = df['contact'].replace({'Y': 1, 'N': 0})\n",
    "\n",
    "df['researchOutcome'] = df['researchOutcome'].astype('category')\n",
    "\n",
    "df.drop(columns=['firstName', 'lastName', 'sighting', 'reportedTimestamp', 'eventDate', 'eventTime'], inplace=True)\n",
    "\n",
    "# Let's one-hot the weather and shape attribute\n",
    "df = pd.get_dummies(df, columns=['weather', 'shape'])\n",
    "\n",
    "# Let's replace the researchOutcome values with 0, 1, 2 for Unexplained, Explained, and Probable\n",
    "df['researchOutcome'] = df['researchOutcome'].replace({'unexplained': 0, 'explained': 1, 'probable': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Creating and training our model (Linear Learner)\n",
    "\n",
    "Let's evaluate the Linear Learner algorithm as well. Let's go ahead and randomize the data again and get it ready for the Linear Leaner algorithm. We will also rearrange the columns so it is ready for the algorithm (it expects the first column to be the target attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rand_split = np.random.rand(len(df))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = rand_split >= 0.9\n",
    "\n",
    " # This dataset will be used to train the model.\n",
    "data_train = df[train_list]\n",
    "\n",
    "# This dataset will be used to validate the model.\n",
    "data_val = df[val_list]\n",
    "\n",
    "# This dataset will be used to test the model.\n",
    "data_test = df[test_list]\n",
    "\n",
    "# Breaks the datasets into attribute numpy.ndarray and the same for target attribute.  \n",
    "train_X = data_train.drop(columns='researchOutcome').values\n",
    "train_y = data_train['researchOutcome'].values\n",
    "\n",
    "val_X = data_val.drop(columns='researchOutcome').values\n",
    "val_y = data_val['researchOutcome'].values\n",
    "\n",
    "test_X = data_test.drop(columns='researchOutcome').values\n",
    "test_y = data_test['researchOutcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's create recordIO file for the training data and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'ufo_sightings_train_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_train/{}'.format(train_file)).upload_fileobj(f)\n",
    "training_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_train/{}'.format(bucket, train_file)\n",
    "print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create recordIO file for the validation data and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_validation/{}'.format(validation_file)).upload_fileobj(f)\n",
    "validate_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_validation/{}'.format(bucket, validation_file)\n",
    "print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Alright we are good to go for the Linear Learner algorithm. Let's get everything we need from the ECR repository to call the Linear Learner algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "container = image_uris.retrieve('linear-learner', boto3.Session().region_name, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training job name\n",
    "job_name = 'ufo-linear-learner-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))\n",
    "\n",
    "# Here is where the model-artifact will be stored\n",
    "output_location = 's3://{}/implementation_operations_lab/linearlearner_output'.format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start building out our model by using the SageMaker Python SDK and passing in everything that is required to create a Linear Learner model.\n",
    "\n",
    "First I like to always create a specific job name. Next, we'll need to specify training parameters.\n",
    "\n",
    "Finally, after everything is included and ready, then we can call the `.fit()` function which specifies the S3 location for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "# Setup the LinearLeaner algorithm from the ECR container\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       instance_count=1, \n",
    "                                       instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess,\n",
    "                                       input_mode='Pipe')\n",
    "# Setup the hyperparameters\n",
    "linear.set_hyperparameters(feature_dim=22,\n",
    "                           predictor_type='multiclass_classifier',\n",
    "                           num_classes=3\n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here\n",
    "                          )\n",
    "\n",
    "# Launch a training job. This method calls the CreateTrainingJob API call\n",
    "data_channels = {\n",
    "    'train': training_recordIO_protobuf_location,\n",
    "    'validation': validate_recordIO_protobuf_location\n",
    "}\n",
    "linear.fit(data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here is the location of the trained Linear Learner model: {}/{}/output/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we have our trained model we can deploy into production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Deploying the model into SageMaker hosting\n",
    "\n",
    "Next, let's deploy the model into SageMaker hosting onto a single m4 instance. We can then use this instance to test the model with the test data that we help out at the beginning of the notebook. We can then evaluate things like accuracy, precision, recall, and f1 score. \n",
    "\n",
    "We can use some fancy libraries to build out a confusion matrix/heatmap to see how accurate our model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multiclass_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code is just setup code to allow us to draw out nice and pretty confusion matrix/heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None, \n",
    "                          cmap=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "            plt.cm.Greens\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Actual',\n",
    "           xlabel='Predicted')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.predictor import json_deserializer, csv_serializer\n",
    "\n",
    "# multiclass_predictor.content_type = 'text/csv'\n",
    "multiclass_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "multiclass_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "predictions = []\n",
    "results = multiclass_predictor.predict(test_X)\n",
    "predictions += [r['predicted_label'] for r in results['predictions']]\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.4)\n",
    "\n",
    "y_test = test_y\n",
    "y_pred = predictions\n",
    "\n",
    "class_names = np.array(['Unexplained', 'Explained', 'Probable'])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix',\n",
    "                      cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test = data_test['researchOutcome']\n",
    "y_pred = predictions\n",
    "scores = precision_recall_fscore_support(y_test, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: {}'.format(acc))\n",
    "print('Precision is: {}'.format(scores[0]))\n",
    "print('Recall is: {}'.format(scores[1]))\n",
    "print('F1 score is: {}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
